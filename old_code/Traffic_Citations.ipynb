{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "'''\n",
    "\n",
    "__author__ = \"Bijan Vafaei\"\n",
    "__copyright__ = \"Copyright 2019, Data Science Assessment\"\n",
    "__credits__ = \n",
    "__license__ = \n",
    "__version__ = \"1.0.2\"\n",
    "__maintainer__ = \n",
    "__email__ = \"bvafaei@epsteinglobal.com\"\n",
    "__status__ = \"Prototype\"\n",
    "\n",
    "'''\n",
    "\n",
    "# Importing required libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bijan\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (0,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Reading the Data from csv file and loading into a pandas dataframe\n",
    "\n",
    "df_parking_citations = pd.DataFrame(data = pd.read_csv('input/parking_citations.corrupted.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Cleansing\n",
    "\n",
    "# Dropping Rows With Missing Values\n",
    "# Description: \n",
    "    # Drop the datapoint with Null value in the following 11 columns. These columns  have less than 40'000 null value\n",
    "    # Effecting less than 1% of the whole training and validation data points\n",
    "\n",
    "df_parking_citations = df_parking_citations.dropna(     axis = 'index', \n",
    "                                                        subset = \n",
    "                                                                ['Issue time', \n",
    "                                                                 'RP State Plate', \n",
    "                                                                 'Body Style', \n",
    "                                                                 'Color', \n",
    "                                                                 'Location',\n",
    "                                                                 'Route',\n",
    "                                                                 'Agency', \n",
    "                                                                 'Violation Description', \n",
    "                                                                 'Fine amount', \n",
    "                                                                 'Latitude', \n",
    "                                                                 'Longitude']\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Cleansing\n",
    "\n",
    "# Dropping Columns Meter Id, Marked Time, and VIN due to high percentage of Missing Values\n",
    "\n",
    "df_parking_citations = df_parking_citations.drop(   columns = \n",
    "                                                                 ['Meter Id',\n",
    "                                                                  'Marked Time',\n",
    "                                                                  'VIN'                                            \n",
    "                                                                 ]\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Cleansing\n",
    "\n",
    "# Replacing Plate Expiry Date with a valid constant value\n",
    "# Filling the null values with the constant of 2020-12 assuming the States have 3 years expiry date program\n",
    "    \n",
    "df_parking_citations['Plate Expiry Date'].fillna(value= 202012.00, inplace= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Cleansing\n",
    "\n",
    "# Primary Key (Ticket number) contains a letter D at the end of them which \n",
    "# shows that those ticket number were deleted or meant to be deleted\n",
    "# Remove those letter for the porpuse of this analytics\n",
    "\n",
    "df_parking_citations['Ticket number'] = df_parking_citations['Ticket number'].astype(str).str.replace('D', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation and Cleansing\n",
    "\n",
    "# Change the data type from object to time stamp for Issue Date\n",
    "# TODO: with Issue Date we can identify day of week, weekend or weekdays, and holiday to expand \n",
    "\n",
    "df_parking_citations['Issue Date'] =  pd.to_datetime(df_parking_citations['Issue Date'], format='%Y-%m-%dT%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representation Transformation\n",
    "\n",
    "# Create the additional variable 'Time of Day' to identify the ticket issuance time of day: \n",
    "# Key: AM: Morning ,  PM: Afternoon \n",
    "\n",
    "df_parking_citations['Time of Day'] =  [ 'AM' if x < 1200 else 'PM' for x in df_parking_citations['Issue time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the Categorical Variables \n",
    "\n",
    "categorical_feature_mask = df_parking_citations.dtypes==object\n",
    "\n",
    "\n",
    "# Filter categorical columns using mask and turn it into a list\n",
    "\n",
    "categorical_cols = df_parking_citations.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Removing Make from the list of categorical columns to avoid labeling it\n",
    "categorical_cols.remove('Make')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing\n",
    "# Representation Transformation: Converting categorical features to numeric representation (Encoding Categorical Variables)\n",
    "\n",
    "\n",
    "# Define the encoder object\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "df_encode = df_parking_citations\n",
    "\n",
    "\n",
    "# Apply the lebel encoder on the categorical feature columns\n",
    "\n",
    "df_encode[categorical_cols] = df_encode[categorical_cols].apply(lambda col: le.fit_transform(col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre_processing\n",
    "#Normalization of the 'Fine amount'\n",
    "\n",
    "x= df_parking_citations['Fine amount']\n",
    "\n",
    "# Normalization to using Min-Max Scaler\n",
    "df_encode['Fine amount'] = (x-x.min())/(x.max()-x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index selection: The uncorrupted subset of the input goes for training and testing\n",
    "# The  data-points with the Make feature will be used for training and testing\n",
    "\n",
    "df_train_test = df_encode.dropna(axis = 'index', subset = ['Make'])\n",
    "\n",
    "\n",
    "\n",
    "# Separating the data-points that the Make feature has been accidentally deleted and need to be predicted\n",
    "\n",
    "df_predict = df_encode[df_encode['Make'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bijan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\bijan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Aggregation: \n",
    "# Identifying the top 25 common Make using groupby and aggregation methods\n",
    "\n",
    "df_common_make = df_train_test[['Ticket number','Make']].groupby(['Make']) \\\n",
    "                                                            .agg('count') \\\n",
    "                                                            .sort_values('Ticket number', ascending = False) \\\n",
    "                                                            .head(25)\\\n",
    "                                                            .reset_index()\\\n",
    "                                                            \n",
    "l_common_make = df_common_make['Make'].tolist()\n",
    "\n",
    "df_train_test ['Common make'] = [1 if x in l_common_make else 0 for x in df_train_test['Make']]\n",
    "\n",
    "\n",
    "df_predict['Common make'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# Split data into training and testing sets\n",
    "\n",
    "X_train_test = df_train_test.drop(['Issue Date','Make', 'Common make'],axis =1)\n",
    "y_train_test = df_train_test['Common make']\n",
    "\n",
    "# Returning labeled and prediction features for training and test\n",
    "# Library from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_test,y_train_test, random_state = 200, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning labeled and prediction features for the subset of the input that requires the Make feature to be predicted\n",
    "\n",
    "X_predict = df_predict.drop(['Issue Date','Make', 'Common make'],axis =1)\n",
    "y_predict = df_predict['Common make']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=100, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN Model\n",
    "\n",
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 100)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the K-fold Cross Validation instead of train-test_split method\n",
    "\n",
    "kfold = KFold(n_splits=5, random_state=None)\n",
    "\n",
    "#train model with cv of 5 \n",
    "cv_scores = cross_val_score(knn, X_train_test, y_train_test, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction of new observation\n",
    "\n",
    "y1_predict = knn.predict(X1_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the probability of the new observation being among top 25 common Make\n",
    "proba_predict1 = knn.predict_proba(X1_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The probability of the new observation being made by one of the top 25 common Make\n",
    "Common_make_proba_predict1 = proba_predict1.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
